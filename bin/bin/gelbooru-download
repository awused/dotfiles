#! /usr/bin/env bash
# Takes a directory and a series of gelbooru urls and downloads them
# Assumes all images are connected, and uses the hash of the first image.
# If the first image's hash is abcd the images will be named abcd01, abcd02, abcd03, etc.
# Skips any images that already exist but will continue to download those that don't
# Doesn't add numbering for single images.

declare -A tagdirs
tagdirs[touhou]="Touhou"
tagdirs[kantai_collection]="Kantai Collection"
tagdirs[strike_witches]="Strike Witches"
tagdirs['neptune_\(series\)']="neptunia"
tagdirs[pokemon]="pokemon"
tagdirs[idolmaster]="im@s"


directory="$1"
counter=1

if [[ ! -d $directory ]]; then
  echo "Directory $directory does not exist"
  exit 1
fi

shift


url=$1
id=$(echo "$url" | grep -Eo 'id\\?=([0-9]+)' | sed -rn 's/.*=([0-9]+).*/\1/p')
if [[ -z $id ]]; then
  echo "No valid gelbooru url found"
  notify-send -u critical "No valid gelbooru url found"
  exit 1
fi
shift

api="https://gelbooru.com/index.php?page=dapi&s=post&q=index&json=1&id="

# If there is no second image, just download this one without a suffix.

response=$(curl -s "$api$id")
tags=$(echo "$response" | jq -r '.post[0].tags')
if [[ $tags == "null" ]]; then
  page=$(curl -s "$url")

  tags=$(echo "$page" | \
    xmllint --html --xpath "//ul[@id='tag-list']//a[contains(@href, '&tags=')]/@href" - 2>/dev/null | \
    sed -E 's/^.*tags=(.*)"$/\1/' | \
    python -c "import sys, urllib.parse as parse; print(parse.unquote(sys.stdin.read()))" | \
    tr '\n' ' ')
fi

for tag in "${!tagdirs[@]}"; do
  if [[ $tags =~ ( |^|\n)${tag}( |$|\n) ]]; then
    directory="${directory}/${tagdirs[$tag]}" 
    break
  fi
done

if [[ ! -d $directory ]]; then
  echo "Creating sub-directory $directory"
  mkdir -p "$directory"
fi

hash=$(echo "$response" | jq -r '.post[0].md5')
image=$(echo "$response" | jq -r '.post[0].image')
fileurl=$(echo "$response" | jq -r '.post[0].file_url')

if [[ $hash == "null" ]]; then
  fileurl=$(echo "$page" | xmllint --html --xpath "//a[text() = 'Original image']/@href" - 2>/dev/null)
  fileurl="${fileurl#*\"}"
  fileurl="${fileurl::-1}"

  image=$(echo "$fileurl" | grep -Eo "[^/]*$")
  hash=$(echo "$image" | grep -Eo "^[^.]+")
fi

if [[ $hash == "" ]]; then
  echo "Couldn't find image URL or hash"
  notify-send -u critical "Couldn't find image URL or hash for $url"
  exit 1
fi

ext="${image##*.}"
filename="$directory/${hash}$(printf '%02d' $counter).$ext"

if [[ -z $1 ]]; then
  filename="$directory/$hash.$ext"
fi

if [[ -f $filename ]]; then
  echo "File already exists: $filename"
else
  echo "Downloading $filename from $fileurl"
  curl -s -o "$filename" "$fileurl" || notify-send -u critical "Download failed for $filename"
  chmod 0664 "$filename" || true
fi

while [[ -n $1 ]]; do
  url=$1
  shift
  id=$(echo "$url" | grep -Eo 'id\\?=([0-9]+)' | sed -rn 's/.*=([0-9]+).*/\1/p')
  if [ -z "$id" ]; then
    echo "Invalid URL $url"
    notify-send -u critical "Invalid URL $url"
    continue
  fi

  let counter++


  response=$(curl -s "$api$id")
  image=$(echo "$response" | jq -r '.post[0].image')
  fileurl=$(echo "$response" | jq -r '.post[0].file_url')

  if [[ $fileurl == "null" ]]; then
    page=$(curl -s "$url")
    fileurl=$(echo "$page" | xmllint --html --xpath "//a[text() = 'Original image']/@href" - 2>/dev/null)
    fileurl="${fileurl#*\"}"
    fileurl="${fileurl::-1}"

    image=$(echo "$fileurl" | grep -Eo "[^/]*$")
  fi

  ext="${image##*.}"
  filename="$directory/${hash}$(printf '%02d' $counter).$ext"

  if [[ -f $filename ]]; then
    echo "File already exists: $filename"
  else
    echo "Downloading $filename from $fileurl"
    curl -s -o "$filename" "$fileurl" || notify-send -u critical "Download failed for $filename"
    chmod 0664 "$filename" || true
  fi
done
